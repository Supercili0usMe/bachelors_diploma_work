{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение качества моделей\n",
    "### Содержание\n",
    "\n",
    "* [Подготовка данных](#1)\n",
    "* [Сравнение моделей](#2)\n",
    "    * [Теоретические основы](#21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вначале нужно определить, на каких данных будет обучаться модель, а на каких тестироваться. **Традиционный подход** - это разделение исходного набора данных на 3 части (обучение, валидация и тестирование) с пропорции 60/20/20. В данном случае обучающая выборка используется для обучения модели, а валидация и тестирование для получения значения метрики без эффекта переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако существует и другой подход к разбиению данных - разделение на 2 части (обучение и тестирование) по правилу 80-20 (80% тренировочный, 20% тестовый). Зачастую данный метод применяется в тех случаях, когда отсутствует достаточное количество данных как в обучающем, так и в проверочном наборе.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем как начать разбивать данные необходимо выделить из исходного набора данных целевую переменную (столбец `DRK_YN`) и сохранить её в отдельную переменную. Ниже приведён код разделения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=[\"DRK_YN\"], axis=1)\n",
    "y = df[\"DRK_YN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе выполнения данной работы будет использован подход с разделением исходной выборки на 2 части с пропорцией 80-20, поскольку данный способ является самым популярным способом разбиения данных. Для того, чтобы разбить данные таким образом, существует специальный метод `train_test_split` в библиотеке `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение моделей <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теоретические основы <a id=\"21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как модели были обучены, их необходимо оценить при помощи имеющихся метрик оценки. Всего для задач классификации существует множество оценок. Но все они строятся на основе **матрицы ошибок** (*Confusion Matrix*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix состоит из 4 основных исходов бинарной классификации: *истинно положительных* (**TP**), *ложно положительных* (**FP**), *истнно отрицательных* (**TN**) и *ложно отрицательных* (**FN**). \n",
    "\n",
    "Пусть какой-то набор медицинских данных характерен для данного диагноза. Если наша модель верно определила и поставила положительный класс, тогда это *истинно положительный* исход, если же модель ставит отрицательную метку класса, тогда это *ложно отрицательный* исход. В случае отсутствия диагноза у рассматриваемого набора данных исходы модели остаются аналогичными. Тогда, если модель относит запись к классу положительному, то мы говорим о *ложно положительном* исходе (здоровому человеку ошибочно поставлен диагноз \"болен\"), и наоборот, если модель определяет запись как отрицательный класс, то от *истинно отрицательный* исход (больному человеку поставлен диагноз \"болен\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из наиболее простых метрик, построенных на основе матрицы ошибок является **полнота** (*Recall*). Полнота определяет долю верных прогнозов среди всех данных с положительным классом. Она определяется отношением истинно положительных результатов к общему количеству положительных результатов. Таким образом формула выглядит следующим образом:\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "В исходной задачи необходимо уделить данной задаче некоторое внимание, поскольку в задаче ошибка нераспознания положительного класса высока."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако одна полнота не способна в полной мере оценить точность классификации. Поэтому вместе с ней применяют и другую метрику - **точность** (*Precision*), которая измеряет, насколько хорошо модель справляется с ложно положительными результатами. Более формально, точность принимает в расчет только те точки данных, которые были помечены как положительные, и определяет, сколько среди них истинно положительные. Формула для подсчета данного значения выглядит следующим образом:\n",
    "$$Precision = \\frac{TP}{TP+FP}$$\n",
    "Данная оценка также является важной, поскольку в рамках данной задачи \"цена\" ложно положительного класса также высока."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, поскольку ранее было выяснено, что метрики **полнота** и **точность** имеют равную высокую значимость, то лучше всего использовать **F-меру** (*F-score*), которая является средним гармоническим между первой и второй метриками. Почему при этом используется именно среднее гармоническое, а не среднее арифметическое? Потому что если одна метрика очень большая, например 100%, а другая наоборот крайне низкая (50%), то среднее арифметическое будет равняться 75%, что является приемлемым результатом. Однако в реальность такой классификатор имеет крайне низкую точность классификации. Таким образом, формула для подсчета выглядит так:\n",
    "$$F_1 = \\frac{2 \\cdot precision \\cdot recall}{precision+recall}$$\n",
    "Если обе метрики большие, то и итоговый результат также высокий. Цель F1-меры состоит в том, чтобы определить, является ли высокой как полнота, так и точность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако иногда требуется, чтобы либо полнота, либо точность превалировала одна над другой. Таким образом, может потребоваться при вычислении F-меры придать одной из них больший вес. Как раз для этого и была придумана $F_\\beta$-мера, формула которой представлена ниже:\n",
    "$$F_\\beta = \\frac{(1+\\beta^2) \\cdot precision \\cdot recall}{\\beta^2 \\cdot precision + recall}$$\n",
    "В данной формуле параметр $\\beta$ может принимать любое положительное значение и обозначает уровень значимости одного из критериев. Чем ниже $\\beta$, тем более весомой становится точность, а чем выше $\\beta$, тем важнее становится полнота."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из ранее рассмотренных метрик становится очевидно, что существует два типа ошибок: ложно положительные и ложно отрицательные. И существует метрика, которая основана на двух данных ошибках - **кривая рабочей характеристики приемника** (*ROC*). Данная кривая строится на основе двух показателей: **избирательности** (*False Positive Rate (FPR)*) и **полноте** (*True Positive Rate (TPR)*). Данные показатели высчитываются по следующим формулам:\n",
    "$$ FPR = \\frac{FP}{FP + TN} $$\n",
    "$$ TPR = \\frac{TP}{TP + TN} $$\n",
    "И далее, на основе данных двух метрик строится ROC-кривая, где по горизонтали откладывается FPR, а по вертикали - TPR. Однако сама по себе ROC-кривая может лишь графически показать, какая модель лучше справляется с классификацией. Для того, чтобы сравнить две модели численно, необходимо найти **площадь под ROC-кривой** (*Area Under Curve ROC (AUC ROC)*). Диапазон значений AUC ROC равен $[0.5, 1]$, где чем ближе к 1, тем лучше модель для классификации данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическая реализация <a id=\"22\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
