{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение качества моделей\n",
    "### Содержание\n",
    "\n",
    "* [Подготовка данных](#1)\n",
    "* [Сравнение моделей](#2)\n",
    "    * [Теоретические основы](#21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вначале нужно определить, на каких данных будет обучаться модель, а на каких тестироваться. **Традиционный подход** - это разделение исходного набора данных на 3 части (обучение, валидация и тестирование) с пропорции 60/20/20. В данном случае обучающая выборка используется для обучения модели, а валидация и тестирование для получения значения метрики без эффекта переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако существует и другой подход к разбиению данных - разделение на 2 части (обучение и тестирование) по правилу 80-20 (80% тренировочный, 20% тестовый). Зачастую данный метод применяется в тех случаях, когда отсутствует достаточное количество данных как в обучающем, так и в проверочном наборе.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем как начать разбивать данные необходимо выделить из исходного набора данных целевую переменную (столбец `DRK_YN`) и сохранить её в отдельную переменную. Ниже приведён код разделения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=[\"DRK_YN\"], axis=1)\n",
    "y = df[\"DRK_YN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе выполнения данной работы будет использован подход с разделением исходной выборки на 2 части с пропорцией 80-20, поскольку данный способ является самым популярным способом разбиения данных. Для того, чтобы разбить данные таким образом, существует специальный метод `train_test_split` в библиотеке `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение моделей <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теоретические основы <a id=\"21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как модели были обучены, их необходимо оценить при помощи имеющихся метрик оценки. Всего для задач классификации существует множество оценок. Но все они строятся на основе **матрицы ошибок** (*Confusion Matrix*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix состоит из 4 основных исходов бинарной классификации: *истинно положительных* (**TP**), *ложно положительных* (**FP**), *истнно отрицательных* (**TN**) и *ложно отрицательных* (**FN**). \n",
    "\n",
    "Пусть какой-то набор медицинских данных характерен для данного диагноза. Если наша модель верно определила и поставила положительный класс, тогда это *истинно положительный* исход, если же модель ставит отрицательную метку класса, тогда это *ложно отрицательный* исход. В случае отсутствия диагноза у рассматриваемого набора данных исходы модели остаются аналогичными. Тогда, если модель относит запись к классу положительному, то мы говорим о *ложно положительном* исходе (здоровому человеку ошибочно поставлен диагноз \"болен\"), и наоборот, если модель определяет запись как отрицательный класс, то от *истинно отрицательный* исход (больному человеку поставлен диагноз \"болен\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из наиболее простых метрик, построенных на основе матрицы ошибок является **полнота** (*Recall*). Полнота определяет долю верных прогнозов среди всех данных с положительным классом. Она определяется отношением истинно положительных результатов к общему количеству положительных результатов. Таким образом формула выглядит следующим образом:\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "В исходной задачи необходимо уделить данной задаче некоторое внимание, поскольку в задаче ошибка нераспознания положительного класса высока."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако одна полнота не способна в полной мере оценить точность классификации. Поэтому вместе с ней применяют и другую метрику - **точность** (*Precision*), которая измеряет, насколько хорошо модель справляется с ложно положительными результатами. Более формально, точность принимает в расчет только те точки данных, которые были помечены как положительные, и определяет, сколько среди них истинно положительные. Формула для подсчета данного значения выглядит следующим образом:\n",
    "$$Precision = \\frac{TP}{TP+FP}$$\n",
    "Данная оценка также является важной, поскольку в рамках данной задачи \"цена\" ложно положительного класса также высока."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, поскольку ранее было выяснено, что метрики **полнота** и **точность** имеют равную высокую значимость, то лучше всего использовать **F-меру** (*F-score*), которая является средним гармоническим между первой и второй метриками. Почему при этом используется именно среднее гармоническое, а не среднее арифметическое? Потому что если одна метрика очень большая, например 100%, а другая наоборот крайне низкая (50%), то среднее арифметическое будет равняться 75%, что является приемлемым результатом. Однако в реальность такой классификатор имеет крайне низкую точность классификации. Таким образом, формула для подсчета выглядит так:\n",
    "$$F_1 = \\frac{2 \\cdot precision \\cdot recall}{precision+recall}$$\n",
    "Если обе метрики большие, то и итоговый результат также высокий. Цель F1-меры состоит в том, чтобы определить, является ли высокой как полнота, так и точность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако иногда требуется, чтобы либо полнота, либо точность превалировала одна над другой. Таким образом, может потребоваться при вычислении F-меры придать одной из них больший вес. Как раз для этого и была придумана $F_\\beta$-мера, формула которой представлена ниже:\n",
    "$$F_\\beta = \\frac{(1+\\beta^2) \\cdot precision \\cdot recall}{\\beta^2 \\cdot precision + recall}$$\n",
    "В данной формуле параметр $\\beta$ может принимать любое положительное значение и обозначает уровень значимости одного из критериев. Чем ниже $\\beta$, тем более весомой становится точность, а чем выше $\\beta$, тем важнее становится полнота."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из ранее рассмотренных метрик становится очевидно, что существует два типа ошибок: ложно положительные и ложно отрицательные. И существует метрика, которая основана на двух данных ошибках - **кривая рабочей характеристики приемника** (*ROC*). Данная кривая строится на основе двух показателей: **избирательности** (*False Positive Rate (FPR)*) и **полноте** (*True Positive Rate (TPR)*). Данные показатели высчитываются по следующим формулам:\n",
    "$$ FPR = \\frac{FP}{FP + TN} $$\n",
    "$$ TPR = \\frac{TP}{TP + TN} $$\n",
    "И далее, на основе данных двух метрик строится ROC-кривая, где по горизонтали откладывается FPR, а по вертикали - TPR. Однако сама по себе ROC-кривая может лишь графически показать, какая модель лучше справляется с классификацией. Для того, чтобы сравнить две модели численно, необходимо найти **площадь под ROC-кривой** (*Area Under Curve ROC (AUC ROC)*). Диапазон значений AUC ROC равен $[0.5, 1]$, где чем ближе к 1, тем лучше модель для классификации данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическая реализация <a id=\"22\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед началом оценки всех моделей создадим функцию для оценки модели. На вход данной функции подаётся обученная модель классификатора и тестовый набор данных (`x_test` и `y_test`). Данная функция в качестве результата выведет на экран матрицу неточностей классификатора (*Confusion Matrix*) и ROC-кривую, а под рисунками будут показаны результаты основных метрик оценивания. Код разработанной функции представлен ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "def evaluate_model(model, x_test, y_test, model_name: str) -> None:\n",
    "    pred = model.predict(x_test)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(model, x_test, y_test, cmap=plt.cm.Blues)\n",
    "    disp.ax_.set_title(f'Confusion matrix of {model_name}')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    disp = RocCurveDisplay.from_estimator(model, x_test, y_test)\n",
    "    disp.ax_.set_title(f'ROC Curve of {model_name}')\n",
    "\n",
    "    plt.show()\n",
    "    vals = f'''Оценивание модели {model_name} по параметрам:\n",
    "    Полнота (Recall): {recall_score(y_test, pred):.4f},\n",
    "    Точность (Precision): {precision_score(y_test, pred):.4f},\n",
    "    F1-мера (F1-score): {f1_score(y_test, pred):.4f},\n",
    "    Площадь под ROC-кривой (AUC ROC): {roc_auc_score(y_test, pred):.4f}'''\n",
    "    print(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее создадим классификационные модели на основе полученных ранее оптимальных параметров при кросс-валидации. Сперва импортируем все необходимые компоненты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее создадим функцию, благодаря которой будет создан классификатор на основе введённого в неё алгоритма. На вход данная функция будет получать алгоритм классификации с введёнными в него параметрами, а на выходе будет получаться полностью обученные классификатор. Программный код разработанной функции представлен ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(alg, alg_name, x_train, y_train):\n",
    "    print(\"Обучение модели...\")\n",
    "    # Создаем стандартизатор\n",
    "    standardizer = StandardScaler()\n",
    "\n",
    "    # Создаём конвейер\n",
    "    pipe = Pipeline([(\"standardizer\", standardizer), (alg_name, alg)])\n",
    "\n",
    "    clf = pipe.fit(x_train, y_train)\n",
    "    print(\"Обучение завершено!\")\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее определим словарь, в котором ключем будет являться название метода классификации, а значением - сам метод классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для стака моделей\n",
    "estimators = [\n",
    "    ('forest', RandomForestClassifier(n_jobs=-1, n_estimators=793, random_state=42, max_depth=14, max_features=12, min_samples_leaf=17)),\n",
    "    ('lr', LogisticRegression(solver='saga', n_jobs=-1, C=1.6, l1_ratio=0.60, penalty=\"elasticnet\"))\n",
    "]\n",
    "\n",
    "algs = {\n",
    "    'k-nearest neighbors': KNeighborsClassifier(n_jobs=-1, metric='manhattan', n_neighbors=14),\n",
    "    'support vector machine': LinearSVC(max_iter=10000, C=1.5, loss=\"squared_hinge\", tol=0.005001),\n",
    "    'logistic regression': LogisticRegression(solver=\"saga\", n_jobs=-1, C=4.972235551967773, l1_ratio=0.00387986251009187, penalty='elasticnet'),\n",
    "    'decision tree': DecisionTreeClassifier(min_samples_leaf=50, criterion='log_loss', max_depth=10, max_features=23),\n",
    "    'random forest': RandomForestClassifier(n_jobs=-1, n_estimators=793, max_depth=12, max_features=12, min_samples_leaf=17),\n",
    "    'adaptive boost': AdaBoostClassifier(learning_rate=0.5, n_estimators=159),\n",
    "    #'gradiend boost': XGBClassifier(random_state=42, colsample_bylevel=0.9452050650024018, colsample_bynode=1.0, colsample_bytree=1.0, gamma=5.966638612389175,\n",
    "    #                                learning_rate=0.08893704430415869, max_depth=17, reg_alpha=20.0, reg_lambda=0.0, subsample=1.0),\n",
    "    'stacked model': StackingClassifier(estimators=estimators, final_estimator=DecisionTreeClassifier(min_samples_leaf=10)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее в цикле пройдемся по всем элементам данного словаря и сперва создадим классификатор, а затем оценим его точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Работа с k-nearest neighbors\n",
      "Обучение модели...\n"
     ]
    }
   ],
   "source": [
    "for name, alg in algs.items():\n",
    "    print(f'Работа с {name}')\n",
    "    clf = create_model(alg, name, x_train, y_train)\n",
    "    evaluate_model(clf, x_test, y_test, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
